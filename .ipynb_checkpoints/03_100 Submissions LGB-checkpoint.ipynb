{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678d8f9b-8dac-4717-b402-4edcb4d142ec",
   "metadata": {},
   "source": [
    "# NCAA March Machine Learning Mania 2025 – Competitive Submission Notebook\n",
    "\n",
    "This notebook implements an ensemble pipeline that:\n",
    "1. Computes Elo ratings from historical regular season detailed results.\n",
    "2. Creates training data using an Elo difference (EloDiff) feature and observed game margin.\n",
    "3. Trains separate LightGBM regression models for men's and women's games to predict margin.\n",
    "4. Converts predicted margins to win probabilities using a logistic transform.\n",
    "5. Loops over 100 iterations (each with a different random seed / slight hyperparameter variation) \n",
    "   to generate 100 candidate submission files.\n",
    "\n",
    "*Note: This code is designed for competitive use and is more elaborate than a simple demonstration.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe36e5b-12a1-479c-94ac-8e65f964a56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and default styles set.\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries & Setup\n",
    "import glob, os\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\", font_scale=1.1)\n",
    "\n",
    "print(\"Libraries imported and default styles set.\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf4a266-21ea-4a0e-a8fd-392bf2216209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Cities with shape (502, 3)\n",
      "Loaded Conferences with shape (51, 2)\n",
      "Loaded MConferenceTourneyGames with shape (6491, 5)\n",
      "Loaded MGameCities with shape (84509, 6)\n",
      "Loaded MMasseyOrdinals with shape (5435396, 5)\n",
      "Loaded MNCAATourneyCompactResults with shape (2518, 8)\n",
      "Loaded MNCAATourneyDetailedResults with shape (1382, 34)\n",
      "Loaded MNCAATourneySeedRoundSlots with shape (776, 5)\n",
      "Loaded MNCAATourneySeeds with shape (2558, 3)\n",
      "Loaded MNCAATourneySlots with shape (2519, 4)\n",
      "Loaded MRegularSeasonCompactResults with shape (190771, 8)\n",
      "Loaded MRegularSeasonDetailedResults with shape (116723, 34)\n",
      "Loaded MSeasons with shape (41, 6)\n",
      "Loaded MSecondaryTourneyCompactResults with shape (1809, 9)\n",
      "Loaded MSecondaryTourneyTeams with shape (1836, 3)\n",
      "Loaded MTeamCoaches with shape (13533, 5)\n",
      "Loaded MTeamConferences with shape (13388, 3)\n",
      "Loaded MTeams with shape (380, 4)\n",
      "Loaded MTeamSpellings with shape (1177, 2)\n",
      "Loaded SampleSubmissionStage1 with shape (507108, 2)\n",
      "Loaded SeedBenchmarkStage1 with shape (507108, 2)\n",
      "Loaded WConferenceTourneyGames with shape (6113, 5)\n",
      "Loaded WGameCities with shape (81342, 6)\n",
      "Loaded WNCAATourneyCompactResults with shape (1650, 8)\n",
      "Loaded WNCAATourneyDetailedResults with shape (894, 34)\n",
      "Loaded WNCAATourneySeeds with shape (1676, 3)\n",
      "Loaded WNCAATourneySlots with shape (1713, 4)\n",
      "Loaded WRegularSeasonCompactResults with shape (134961, 8)\n",
      "Loaded WRegularSeasonDetailedResults with shape (79639, 34)\n",
      "Loaded WSeasons with shape (28, 6)\n",
      "Loaded WSecondaryTourneyCompactResults with shape (828, 9)\n",
      "Loaded WSecondaryTourneyTeams with shape (824, 3)\n",
      "Loaded WTeamConferences with shape (9490, 3)\n",
      "Loaded WTeams with shape (378, 2)\n",
      "Loaded WTeamSpellings with shape (1170, 2)\n",
      "\n",
      "All CSV files loaded automatically.\n",
      "\n",
      "Total Execution Time: 2.12 seconds\n"
     ]
    }
   ],
   "source": [
    "input_folder = r\"C:\\Users\\Hi\\My Works\\My Py Scripts\\Git Repos\\48_March-achine-learning-mania-2025\\Input\"\n",
    "csv_files = glob.glob(os.path.join(input_folder, \"*.csv\"))\n",
    "\n",
    "dataframes = {}\n",
    "for file in csv_files:\n",
    "    key = os.path.splitext(os.path.basename(file))[0]\n",
    "    try:\n",
    "        # Using 'latin-1' encoding to avoid Unicode issues\n",
    "        dataframes[key] = pd.read_csv(file, low_memory=False, encoding=\"latin-1\")\n",
    "        print(f\"Loaded {key} with shape {dataframes[key].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "print(\"\\nAll CSV files loaded automatically.\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal Execution Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b58960-430a-4d55-b456-b0752507956b",
   "metadata": {},
   "source": [
    "## Compute Elo Ratings from Historical Regular Season Detailed Results\n",
    "We use the men's and women's regular season detailed results along with the team lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ebbc9a-1516-4792-ae87-f68f5859390b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Elo ratings for men's and women's data.\n",
      "\n",
      "Total Execution Time: 11.17 seconds\n"
     ]
    }
   ],
   "source": [
    "def initialize_elo(team_ids, start_elo=1500):\n",
    "    return {tid: start_elo for tid in team_ids}\n",
    "\n",
    "def update_elo(elo_dict, teamA, teamB, scoreA, scoreB, k=20):\n",
    "    ra = elo_dict[teamA]\n",
    "    rb = elo_dict[teamB]\n",
    "    ea = 1.0 / (1 + 10 ** ((rb - ra) / 400))\n",
    "    sa = 1 if scoreA > scoreB else 0\n",
    "    sb = 1 - sa\n",
    "    elo_dict[teamA] = ra + k * (sa - ea)\n",
    "    elo_dict[teamB] = rb + k * (sb - (1 - ea))\n",
    "\n",
    "def compute_elo(df_games, teams_df):\n",
    "    df_sorted = df_games.sort_values(by=['Season','DayNum'])\n",
    "    team_ids = teams_df['TeamID'].unique()\n",
    "    elo_dict = initialize_elo(team_ids)\n",
    "    for idx, row in df_sorted.iterrows():\n",
    "        update_elo(elo_dict, row['WTeamID'], row['LTeamID'], row['WScore'], row['LScore'])\n",
    "    return elo_dict\n",
    "\n",
    "# Get men's and women's team lists\n",
    "df_MTeams = dataframes['MTeams']\n",
    "df_WTeams = dataframes['WTeams']\n",
    "\n",
    "# Use regular season detailed results for Elo calculation.\n",
    "df_MReg = dataframes['MRegularSeasonDetailedResults'].copy()\n",
    "df_WReg = dataframes['WRegularSeasonDetailedResults'].copy()\n",
    "\n",
    "elo_m = compute_elo(df_MReg, df_MTeams)\n",
    "elo_w = compute_elo(df_WReg, df_WTeams)\n",
    "print(\"Computed Elo ratings for men's and women's data.\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal Execution Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d01b0-1a30-4a24-b2a5-dbfb76ad7ab3",
   "metadata": {},
   "source": [
    "## Prepare Training Data for Margin Modeling\n",
    "For each game, we compute:\n",
    "- **EloDiff**: Difference in Elo ratings between the winning and losing teams.\n",
    "- **Margin**: The observed score margin (WScore - LScore).\n",
    "We do this for both men's and women's historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "533be593-ded6-49a3-b72c-31952b49f95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared training data for men's and women's margin models.\n",
      "Men’s train data shape: (116723, 2)\n",
      "Women’s train data shape: (79639, 2)\n",
      "\n",
      "Total Execution Time: 25.78 seconds\n"
     ]
    }
   ],
   "source": [
    "def prepare_training_data(df, elo_dict):\n",
    "    elo_diffs = []\n",
    "    margins = []\n",
    "    for idx, row in df.iterrows():\n",
    "        # Use final Elo ratings as a proxy (could be improved with dynamic Elo)\n",
    "        diff = elo_dict.get(row['WTeamID'], 1500) - elo_dict.get(row['LTeamID'], 1500)\n",
    "        elo_diffs.append(diff)\n",
    "        margins.append(row['WScore'] - row['LScore'])\n",
    "    return pd.DataFrame({'EloDiff': elo_diffs, 'Margin': margins})\n",
    "\n",
    "train_m = prepare_training_data(df_MReg, elo_m)\n",
    "train_w = prepare_training_data(df_WReg, elo_w)\n",
    "print(\"Prepared training data for men's and women's margin models.\")\n",
    "print(\"Men’s train data shape:\", train_m.shape)\n",
    "print(\"Women’s train data shape:\", train_w.shape)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal Execution Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d4749b-0f20-42a3-9987-65f05f7b0f78",
   "metadata": {},
   "source": [
    "## Train LightGBM Models for Margin Prediction\n",
    "We use LightGBM regressors to predict the margin from the Elo difference.\n",
    "We train separate models for men's and women's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c518c1b4-a876-48df-b258-0cd63fc56ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined train_lgb_model function for GPU-based LightGBM.\n",
      "\n",
      "Total Execution Time: 25.81 seconds\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Train LightGBM (GPU) + Early Stopping\n",
    "\n",
    "def train_lgb_model(df_train, seed=42):\n",
    "    \"\"\"\n",
    "    Trains a LightGBM regressor on EloDiff -> Margin using GPU acceleration & early stopping.\n",
    "    \"\"\"\n",
    "    # Prepare dataset\n",
    "    X = df_train[['EloDiff']]\n",
    "    y = df_train['Margin']\n",
    "    \n",
    "    # Train/val split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=seed\n",
    "    )\n",
    "    \n",
    "    # GPU parameters\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mse',\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 31,\n",
    "        'seed': seed,\n",
    "        'verbose': -1,\n",
    "        'device_type': 'gpu',         # or 'device': 'gpu' in older versions\n",
    "        'tree_learner': 'data_parallel',  # or 'serial'/'gpu'\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0\n",
    "    }\n",
    "    \n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dval   = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        valid_sets=[dval],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "            lgb.log_evaluation(period=0)\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"Defined train_lgb_model function for GPU-based LightGBM.\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal Execution Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa0882-a82e-4807-8cbe-6465344d64e0",
   "metadata": {},
   "source": [
    "## Define Function to Convert Margin to Win Probability\n",
    "We use a logistic transformation with a scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a15de3db-a0e9-4346-bb88-338715d6c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_to_probability(margin, scale=10.0):\n",
    "    \"\"\"\n",
    "    Logistic transform to convert predicted margin -> win probability.\n",
    "    P = 1 / [1 + 10^(-margin/scale)]\n",
    "    \"\"\"\n",
    "    return 1.0 / (1 + 10 ** (-margin / scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013764ee-e0ce-4497-a762-4eeca4406c84",
   "metadata": {},
   "source": [
    "## Prepare Test Data from the Sample Submission File\n",
    "We parse the submission ID to extract Season, Team1, and Team2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff30e2d3-c0b9-4465-9a87-8202d72375a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data prepared from the submission file.\n",
      "               ID  Pred  Season  Team1  Team2\n",
      "0  2021_1101_1102   0.5    2021   1101   1102\n",
      "1  2021_1101_1103   0.5    2021   1101   1103\n",
      "2  2021_1101_1104   0.5    2021   1101   1104\n",
      "3  2021_1101_1105   0.5    2021   1101   1105\n",
      "4  2021_1101_1106   0.5    2021   1101   1106\n"
     ]
    }
   ],
   "source": [
    "df_sub = dataframes['SampleSubmissionStage1'].copy()\n",
    "\n",
    "def parse_id(match_id):\n",
    "    season, t1, t2 = match_id.split('_')\n",
    "    return int(season), int(t1), int(t2)\n",
    "\n",
    "df_sub['Season'] = df_sub['ID'].apply(lambda x: parse_id(x)[0])\n",
    "df_sub['Team1'] = df_sub['ID'].apply(lambda x: parse_id(x)[1])\n",
    "df_sub['Team2'] = df_sub['ID'].apply(lambda x: parse_id(x)[2])\n",
    "print(\"Test data prepared from the submission file.\")\n",
    "print(df_sub.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e96f8a1-479e-4b42-9c58-45b6d0538e8e",
   "metadata": {},
   "source": [
    "## Define a Function to Predict the Outcome for a Given Match\n",
    "For each matchup, we compute the Elo difference using the final Elo ratings,\n",
    "predict the margin using the corresponding LightGBM model (men's or women's),\n",
    "and convert that margin to a win probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f04636-7cdd-4678-9357-f71cf835cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_match(season, team1, team2,\n",
    "                  elo_m, elo_w,\n",
    "                  model_m, model_w):\n",
    "    \"\"\"\n",
    "    If both teams < 2000 => men’s\n",
    "    If both teams >= 3000 => women’s\n",
    "    Otherwise default prob=0.5\n",
    "    \"\"\"\n",
    "    if team1 < 2000 and team2 < 2000:\n",
    "        e1 = elo_m.get(team1, 1500)\n",
    "        e2 = elo_m.get(team2, 1500)\n",
    "        elo_diff = e1 - e2\n",
    "        margin_pred = model_m.predict(pd.DataFrame({'EloDiff': [elo_diff]}))[0]\n",
    "        prob = margin_to_probability(margin_pred)\n",
    "    elif team1 >= 3000 and team2 >= 3000:\n",
    "        e1 = elo_w.get(team1, 1500)\n",
    "        e2 = elo_w.get(team2, 1500)\n",
    "        elo_diff = e1 - e2\n",
    "        margin_pred = model_w.predict(pd.DataFrame({'EloDiff': [elo_diff]}))[0]\n",
    "        prob = margin_to_probability(margin_pred)\n",
    "    else:\n",
    "        # If \"mixed\" or out-of-range, fallback to 0.5\n",
    "        prob = 0.5\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f6de92-2d87-4c6d-b705-c130e99f1759",
   "metadata": {},
   "source": [
    "## Generate Ensemble Predictions & Save 100 Submission Files\n",
    "We loop over 100 iterations. In each iteration, we slightly vary the model seeds (and thus hyperparameters)\n",
    "by retraining the LightGBM models using a different random seed. We then predict for every matchup in the test set,\n",
    "and save the resulting predictions to a separate CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ace05de7-7e25-4306-87d0-1406dcb043c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating submission 1/100 with seed 1001 ===\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[410]\tvalid_0's l2: 73.3057\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[518]\tvalid_0's l2: 101.719\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m t1 \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     26\u001b[0m t2 \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 27\u001b[0m p  \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melo_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melo_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_m_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_w_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Optional random perturbation for diversity\u001b[39;00m\n\u001b[0;32m     30\u001b[0m p \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.005\u001b[39m)   \u001b[38;5;66;03m# Tiny noise\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m, in \u001b[0;36mpredict_match\u001b[1;34m(season, team1, team2, elo_m, elo_w, model_m, model_w)\u001b[0m\n\u001b[0;32m     17\u001b[0m     e2 \u001b[38;5;241m=\u001b[39m elo_w\u001b[38;5;241m.\u001b[39mget(team2, \u001b[38;5;241m1500\u001b[39m)\n\u001b[0;32m     18\u001b[0m     elo_diff \u001b[38;5;241m=\u001b[39m e1 \u001b[38;5;241m-\u001b[39m e2\n\u001b[1;32m---> 19\u001b[0m     margin_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_w\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEloDiff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43melo_diff\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     20\u001b[0m     prob \u001b[38;5;241m=\u001b[39m margin_to_probability(margin_pred)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# If \"mixed\" or out-of-range, fallback to 0.5\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\lightgbm\\basic.py:4748\u001b[0m, in \u001b[0;36mBooster.predict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features, **kwargs)\u001b[0m\n\u001b[0;32m   4746\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4747\u001b[0m         num_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 4748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_has_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_has_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4757\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\lightgbm\\basic.py:1185\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pred_for_csc(\n\u001b[0;32m   1179\u001b[0m         csc\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m   1180\u001b[0m         start_iteration\u001b[38;5;241m=\u001b[39mstart_iteration,\n\u001b[0;32m   1181\u001b[0m         num_iteration\u001b[38;5;241m=\u001b[39mnum_iteration,\n\u001b[0;32m   1182\u001b[0m         predict_type\u001b[38;5;241m=\u001b[39mpredict_type,\n\u001b[0;32m   1183\u001b[0m     )\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m-> 1185\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pred_for_np2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_pyarrow_table(data):\n\u001b[0;32m   1192\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pred_for_pyarrow_table(\n\u001b[0;32m   1193\u001b[0m         table\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m   1194\u001b[0m         start_iteration\u001b[38;5;241m=\u001b[39mstart_iteration,\n\u001b[0;32m   1195\u001b[0m         num_iteration\u001b[38;5;241m=\u001b[39mnum_iteration,\n\u001b[0;32m   1196\u001b[0m         predict_type\u001b[38;5;241m=\u001b[39mpredict_type,\n\u001b[0;32m   1197\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\lightgbm\\basic.py:1344\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d\u001b[1;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preds, nrow\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_predict_np2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1350\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\lightgbm\\basic.py:1291\u001b[0m, in \u001b[0;36m_InnerPredictor.__inner_predict_np2d\u001b[1;34m(self, mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length of pre-allocated predict array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1289\u001b[0m out_num_preds \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int64(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1290\u001b[0m _safe_call(\n\u001b[1;32m-> 1291\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterPredictForMat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mptr_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_ptr_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_C_API_IS_ROW_MAJOR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_c_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_parameter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_num_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOINTER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_double\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1305\u001b[0m )\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_preds \u001b[38;5;241m!=\u001b[39m out_num_preds\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length for predict results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "# We'll generate 100 submissions.\n",
    "num_submissions = 100\n",
    "\n",
    "# Create a folder to save submissions (if not exists)\n",
    "submission_folder = \"ensemble_submissions\"\n",
    "os.makedirs(submission_folder, exist_ok=True)\n",
    "\n",
    "for i in range(1, num_submissions + 1):\n",
    "    # Use a new seed for each iteration\n",
    "    seed_val = 1000 + i\n",
    "    print(f\"\\n=== Generating submission {i}/{num_submissions} with seed {seed_val} ===\")\n",
    "    \n",
    "    # 1) Retrain men’s model\n",
    "    model_m_i = train_lgb_model(train_m, seed=seed_val)\n",
    "    # 2) Retrain women’s model\n",
    "    model_w_i = train_lgb_model(train_w, seed=seed_val)\n",
    "    \n",
    "    # 3) Predict for each row in sample submission\n",
    "    preds = []\n",
    "    for idx, row in df_sub.iterrows():\n",
    "        s  = row['Season']\n",
    "        t1 = row['Team1']\n",
    "        t2 = row['Team2']\n",
    "        p  = predict_match(s, t1, t2, elo_m, elo_w, model_m_i, model_w_i)\n",
    "        \n",
    "        # Optional random perturbation for diversity\n",
    "        p += np.random.normal(0, 0.005)   # Tiny noise\n",
    "        p = np.clip(p, 0.001, 0.999)      # Keep in [0.001, 0.999]\n",
    "        \n",
    "        preds.append(p)\n",
    "    \n",
    "    # 4) Assign predictions & save\n",
    "    df_sub['Pred'] = preds\n",
    "    submission_filename = os.path.join(submission_folder, f\"submission_gpu_{i}.csv\")\n",
    "    df_sub[['ID','Pred']].to_csv(submission_filename, index=False)\n",
    "    print(f\"Saved => {submission_filename}\")\n",
    "   \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal Execution Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nEnsemble submission generation complete. 100 submission files created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a0521d-f1db-49d1-be63-f1f8c5db7714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
