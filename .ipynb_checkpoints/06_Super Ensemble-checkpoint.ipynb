{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a85dc8b-9b9c-47fc-afed-33632ff3335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\", font_scale=1.1)\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fc914d8-bfb4-468d-b7db-f535d2c69612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. (Men’s and Women’s results, teams, and Massey ratings)\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "# Adjust these paths as needed\n",
    "data_folder = \"Input\"\n",
    "\n",
    "# Load men's regular season detailed results and teams\n",
    "m_reg = pd.read_csv(os.path.join(data_folder, \"MRegularSeasonDetailedResults.csv\"))\n",
    "MTeams = pd.read_csv(os.path.join(data_folder, \"MTeams.csv\"))\n",
    "\n",
    "# Load women's regular season detailed results and teams\n",
    "w_reg = pd.read_csv(os.path.join(data_folder, \"WRegularSeasonDetailedResults.csv\"))\n",
    "WTeams = pd.read_csv(os.path.join(data_folder, \"WTeams.csv\"))\n",
    "\n",
    "# Load Massey ratings – we assume it contains a column \"Team\" and \"MAS\"\n",
    "massey_ratings = pd.read_csv(os.path.join(\"Massey Ratings.csv\"))\n",
    "\n",
    "print(\"Data loaded. (Men’s and Women’s results, teams, and Massey ratings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "142d497e-af61-4f56-ae04-f0466239ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New ratings computed for men and women (2019-2023).\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "def compute_team_new_rating(df, team_id, bonus=2):\n",
    "    \"\"\"\n",
    "    Computes the new rating for a team as the average adjusted margin.\n",
    "    Adjust margins using a bonus: if a team is home, add bonus; if away, subtract bonus.\n",
    "    \n",
    "    Parameters:\n",
    "      df (DataFrame): Game results (for seasons 2019-2023).\n",
    "      team_id (int): The team ID.\n",
    "      bonus (float): Points bonus for playing at home.\n",
    "      \n",
    "    Returns:\n",
    "      float: The new rating (average adjusted margin).\n",
    "    \"\"\"\n",
    "    margins = []\n",
    "    \n",
    "    # Games where team was the winner.\n",
    "    df_win = df[df[\"WTeamID\"] == team_id].copy()\n",
    "    # When the winning team was home, add bonus; when away subtract bonus.\n",
    "    df_win[\"adj_margin\"] = df_win[\"WScore\"] - df_win[\"LScore\"]\n",
    "    df_win.loc[df_win[\"WLoc\"] == \"H\", \"adj_margin\"] += bonus\n",
    "    df_win.loc[df_win[\"WLoc\"] == \"A\", \"adj_margin\"] -= bonus\n",
    "    \n",
    "    margins.extend(df_win[\"adj_margin\"].tolist())\n",
    "    \n",
    "    # Games where team was the loser.\n",
    "    df_loss = df[df[\"LTeamID\"] == team_id].copy()\n",
    "    # For losing team, the effective margin is negative.\n",
    "    # Determine team’s home status: if WLoc==\"H\", losing team was away; if WLoc==\"A\", losing team was home.\n",
    "    df_loss[\"adj_margin\"] = -(df_loss[\"WScore\"] - df_loss[\"LScore\"])\n",
    "    df_loss.loc[df_loss[\"WLoc\"] == \"H\", \"adj_margin\"] -= bonus   # losing away: subtract bonus further\n",
    "    df_loss.loc[df_loss[\"WLoc\"] == \"A\", \"adj_margin\"] += bonus   # losing at home: cushion loss\n",
    "    \n",
    "    margins.extend(df_loss[\"adj_margin\"].tolist())\n",
    "    \n",
    "    if len(margins) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(margins)\n",
    "\n",
    "# Compute team IDs from teams files.\n",
    "team_ids_m = MTeams[\"TeamID\"].unique()\n",
    "team_ids_w = WTeams[\"TeamID\"].unique()\n",
    "\n",
    "# Filter results for seasons 2019-2023.\n",
    "m_reg_new = m_reg[(m_reg[\"Season\"] >= 2019) & (m_reg[\"Season\"] <= 2023)]\n",
    "w_reg_new = w_reg[(w_reg[\"Season\"] >= 2019) & (w_reg[\"Season\"] <= 2023)]\n",
    "\n",
    "# Compute new ratings.\n",
    "new_rating_men = {tid: compute_team_new_rating(m_reg_new, tid) for tid in team_ids_m}\n",
    "new_rating_women = {tid: compute_team_new_rating(w_reg_new, tid) for tid in team_ids_w}\n",
    "\n",
    "print(\"New ratings computed for men and women (2019-2023).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efe1af12-ccde-43fc-8ac5-034c42d5668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined ratings computed for men and women.\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "# For simplicity, suppose we already have dictionaries for Elo ratings:\n",
    "# (In practice these would be computed by your Elo code)\n",
    "elo_m = {tid: 1500 for tid in team_ids_m}   # dummy values; replace with your computed Elo ratings\n",
    "elo_w = {tid: 1500 for tid in team_ids_w}\n",
    "\n",
    "# Create a mapping from team name to MAS rating.\n",
    "massey_mapping = dict(zip(massey_ratings[\"Team\"], massey_ratings[\"MAS\"]))\n",
    "\n",
    "# Create a mapping for teams: assume MTeams and WTeams have columns \"TeamID\" and \"TeamName\"\n",
    "MTeams[\"CombinedMAS\"] = MTeams[\"TeamName\"].map(massey_mapping)\n",
    "WTeams[\"CombinedMAS\"] = WTeams[\"TeamName\"].map(massey_mapping)\n",
    "\n",
    "def compute_combined_rating(team_id, gender=\"M\", weight_elo=0.33, weight_new=0.33, weight_massey=0.34):\n",
    "    if gender == \"M\":\n",
    "        # Get ratings from Elo, new, and Massey\n",
    "        rating_elo = elo_m.get(team_id, 1500)\n",
    "        rating_new = new_rating_men.get(team_id, np.nan)\n",
    "        # Find team's MAS rating from MTeams\n",
    "        mas = MTeams.loc[MTeams[\"TeamID\"] == team_id, \"CombinedMAS\"]\n",
    "    else:\n",
    "        rating_elo = elo_w.get(team_id, 1500)\n",
    "        rating_new = new_rating_women.get(team_id, np.nan)\n",
    "        mas = WTeams.loc[WTeams[\"TeamID\"] == team_id, \"CombinedMAS\"]\n",
    "    rating_massey = mas.values[0] if not mas.empty else np.nan\n",
    "    # Compute weighted average (if any rating is missing, use the average of the available ones)\n",
    "    ratings = np.array([rating_elo, rating_new, rating_massey], dtype=float)\n",
    "    weights = np.array([weight_elo, weight_new, weight_massey])\n",
    "    valid = ~np.isnan(ratings)\n",
    "    if valid.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.average(ratings[valid], weights=weights[valid])\n",
    "\n",
    "# Create dictionaries for combined ratings for men and women.\n",
    "combined_rating_men = {tid: compute_combined_rating(tid, \"M\") for tid in team_ids_m}\n",
    "combined_rating_women = {tid: compute_combined_rating(tid, \"W\") for tid in team_ids_w}\n",
    "\n",
    "print(\"Combined ratings computed for men and women.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd5ceef-d253-4dcc-afc5-1e1d4d048a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation data prepared for both men and women.\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "def prepare_model_data(df, combined_rating_dict, gender=\"M\"):\n",
    "    \"\"\"\n",
    "    For each game, compute the feature as the difference in combined ratings.\n",
    "    Target is the game margin (WScore - LScore).\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for idx, row in df.iterrows():\n",
    "        # Only include games where both teams have a combined rating.\n",
    "        if gender == \"M\":\n",
    "            r_w = combined_rating_men.get(row[\"WTeamID\"], np.nan)\n",
    "            r_l = combined_rating_men.get(row[\"LTeamID\"], np.nan)\n",
    "        else:\n",
    "            r_w = combined_rating_women.get(row[\"WTeamID\"], np.nan)\n",
    "            r_l = combined_rating_women.get(row[\"LTeamID\"], np.nan)\n",
    "        if np.isnan(r_w) or np.isnan(r_l):\n",
    "            continue\n",
    "        diff = r_w - r_l\n",
    "        X.append([diff])\n",
    "        y.append(row[\"WScore\"] - row[\"LScore\"])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Split the data by season.\n",
    "train_m = m_reg[m_reg[\"Season\"] < 2024]\n",
    "val_m = m_reg[m_reg[\"Season\"] == 2024]\n",
    "train_w = w_reg[w_reg[\"Season\"] < 2024]\n",
    "val_w = w_reg[w_reg[\"Season\"] == 2024]\n",
    "\n",
    "X_train_m, y_train_m = prepare_model_data(train_m, combined_rating_men, \"M\")\n",
    "X_val_m, y_val_m = prepare_model_data(val_m, combined_rating_men, \"M\")\n",
    "X_train_w, y_train_w = prepare_model_data(train_w, combined_rating_women, \"W\")\n",
    "X_val_w, y_val_w = prepare_model_data(val_w, combined_rating_women, \"W\")\n",
    "\n",
    "print(\"Training and validation data prepared for both men and women.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6931302-4998-4901-8294-767cef3cd55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define a Keras model builder for our DNN.\n",
    "def build_dnn_model(layers=[Dense(32, activation='relu'), Dropout(0.2), Dense(1)]):\n",
    "    model = Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Dictionary for candidate models.\n",
    "models_grid = {\n",
    "    \"LinearRegression\": {\n",
    "        \"model\": LinearRegression(),\n",
    "        \"params\": {}\n",
    "    },\n",
    "    \"Ridge\": {\n",
    "        \"model\": Ridge(),\n",
    "        \"params\": {\"alpha\": uniform(0.1, 10)}\n",
    "    },\n",
    "    \"Lasso\": {\n",
    "        \"model\": Lasso(max_iter=5000),\n",
    "        \"params\": {\"alpha\": uniform(0.001, 1)}\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"model\": DecisionTreeRegressor(),\n",
    "        \"params\": {\"max_depth\": randint(1, 10), \"min_samples_split\": randint(2, 10)}\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        \"params\": {\"n_estimators\": randint(50, 200), \"max_depth\": randint(1, 10)}\n",
    "    },\n",
    "    \"ExtraTrees\": {\n",
    "        \"model\": ExtraTreesRegressor(random_state=42, n_jobs=-1),\n",
    "        \"params\": {\"n_estimators\": randint(50, 200), \"max_depth\": randint(1, 10)}\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"model\": GradientBoostingRegressor(random_state=42),\n",
    "        \"params\": {\"n_estimators\": randint(50, 200), \"learning_rate\": uniform(0.01, 0.3), \"max_depth\": randint(1, 10)}\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": xgb.XGBRegressor(tree_method=\"gpu_hist\", random_state=42, n_jobs=-1),\n",
    "        \"params\": {\"n_estimators\": randint(50, 200), \"learning_rate\": uniform(0.01, 0.3), \"max_depth\": randint(1, 10)}\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        \"model\": lgb.LGBMRegressor(device=\"gpu\", random_state=42),\n",
    "        \"params\": {\"n_estimators\": randint(50, 200), \"learning_rate\": uniform(0.01, 0.3), \"num_leaves\": randint(20, 50)}\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        \"model\": cb.CatBoostRegressor(task_type=\"GPU\", verbose=0, random_state=42),\n",
    "        \"params\": {\"iterations\": randint(50, 200), \"learning_rate\": uniform(0.01, 0.3), \"depth\": randint(3, 10)}\n",
    "    },\n",
    "    \"DNN\": {\n",
    "        \"model\": KerasRegressor(model=build_dnn_model, verbose=0),\n",
    "        \"params\": {\n",
    "            \"model__layers\": [[Dense(32, activation='relu'), Dropout(0.2), Dense(1)],\n",
    "                              [Dense(64, activation='relu'), Dropout(0.3), Dense(1)]],\n",
    "            \"batch_size\": randint(16, 64),\n",
    "            \"epochs\": randint(10, 50)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def tune_and_evaluate(model_obj, param_dist, X_train, y_train, X_val, y_val):\n",
    "    rs = RandomizedSearchCV(model_obj, param_distributions=param_dist, \n",
    "                              n_iter=10, cv=3, scoring=\"neg_mean_squared_error\", random_state=42,\n",
    "                              error_score='raise', n_jobs=-1)\n",
    "    rs.fit(X_train, y_train)\n",
    "    best_model = rs.best_estimator_\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    brier = np.mean((y_pred - 1)**2)  # dummy brier with ground truth assumed = 1\n",
    "    return best_model, rmse, brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f623fdd-6d54-4ea9-8389-90cb4183d032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Men's models:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hi\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Men's LinearRegression: RMSE = 9.0287, Brier = 121.7165\n",
      "Men's Ridge: RMSE = 9.0287, Brier = 121.7165\n",
      "Men's Lasso: RMSE = 9.0303, Brier = 121.6982\n",
      "Men's DecisionTree: RMSE = 8.5194, Brier = 130.8397\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, info \u001b[38;5;129;01min\u001b[39;00m models_grid\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m         best_model, rmse, brier \u001b[38;5;241m=\u001b[39m \u001b[43mtune_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mX_train_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m         men_results[name] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m\"\u001b[39m: rmse, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrier\u001b[39m\u001b[38;5;124m\"\u001b[39m: brier}\n\u001b[0;32m     10\u001b[0m         men_best_models[name] \u001b[38;5;241m=\u001b[39m best_model\n",
      "Cell \u001b[1;32mIn[9], line 69\u001b[0m, in \u001b[0;36mtune_and_evaluate\u001b[1;34m(model_obj, param_dist, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtune_and_evaluate\u001b[39m(model_obj, param_dist, X_train, y_train, X_val, y_val):\n\u001b[0;32m     66\u001b[0m     rs \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(model_obj, param_distributions\u001b[38;5;241m=\u001b[39mparam_dist, \n\u001b[0;32m     67\u001b[0m                               n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     68\u001b[0m                               error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     best_model \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     71\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:936\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    934\u001b[0m refit_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 936\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    938\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "men_results = {}\n",
    "men_best_models = {}\n",
    "print(\"Tuning Men's models:\")\n",
    "for name, info in models_grid.items():\n",
    "    try:\n",
    "        best_model, rmse, brier = tune_and_evaluate(info[\"model\"], info[\"params\"], \n",
    "                                                    X_train_m, y_train_m, X_val_m, y_val_m)\n",
    "        men_results[name] = {\"rmse\": rmse, \"brier\": brier}\n",
    "        men_best_models[name] = best_model\n",
    "        print(f\"Men's {name}: RMSE = {rmse:.4f}, Brier = {brier:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Tuning men's {name} failed: {e}\")\n",
    "\n",
    "print(\"\\nTuning Women's models:\")\n",
    "women_results = {}\n",
    "women_best_models = {}\n",
    "for name, info in models_grid.items():\n",
    "    try:\n",
    "        best_model, rmse, brier = tune_and_evaluate(info[\"model\"], info[\"params\"], \n",
    "                                                    X_train_w, y_train_w, X_val_w, y_val_w)\n",
    "        women_results[name] = {\"rmse\": rmse, \"brier\": brier}\n",
    "        women_best_models[name] = best_model\n",
    "        print(f\"Women's {name}: RMSE = {rmse:.4f}, Brier = {brier:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Tuning women's {name} failed: {e}\")\n",
    "\n",
    "print(\"\\nMen's Model Results:\")\n",
    "print(men_results)\n",
    "print(\"\\nWomen's Model Results:\")\n",
    "print(women_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ee296-dca1-4ea2-a322-4b7d4326228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# For men:\n",
    "val_preds_m = pd.DataFrame({\"ID\": np.arange(len(X_val_m))})\n",
    "for name, model in men_best_models.items():\n",
    "    val_preds_m[name] = model.predict(X_val_m)\n",
    "\n",
    "# For women:\n",
    "val_preds_w = pd.DataFrame({\"ID\": np.arange(len(X_val_w))})\n",
    "for name, model in women_best_models.items():\n",
    "    val_preds_w[name] = model.predict(X_val_w)\n",
    "\n",
    "print(\"Validation predictions generated for both men and women.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0456d-5d5f-4666-9751-d7c51764163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# For demonstration we simulate a small submission DataFrame.\n",
    "# In practice, load your actual submission file.\n",
    "df_sub = pd.DataFrame({\n",
    "    \"ID\": [\"2024_1101_1102\", \"2024_3101_3102\"],\n",
    "    \"Season\": [2024, 2024],\n",
    "    \"Team1\": [1101, 3101],\n",
    "    \"Team2\": [1102, 3102]\n",
    "})\n",
    "\n",
    "def predict_match(row, men_model_name, women_model_name):\n",
    "    s = row[\"Season\"]\n",
    "    t1 = row[\"Team1\"]\n",
    "    t2 = row[\"Team2\"]\n",
    "    # Use corresponding model predictions (here we use our tuned models on validation)\n",
    "    # In production, you would use the models to predict on your test features.\n",
    "    if t1 < 2000 and t2 < 2000:\n",
    "        # Use men’s prediction: we recompute feature diff using combined ratings.\n",
    "        diff = combined_rating_men.get(t1, 1500) - combined_rating_men.get(t2, 1500)\n",
    "        pred = men_best_models[men_model_name].predict(np.array([[diff]]))[0]\n",
    "    elif t1 >= 3000 and t2 >= 3000:\n",
    "        diff = combined_rating_women.get(t1, 1500) - combined_rating_women.get(t2, 1500)\n",
    "        pred = women_best_models[women_model_name].predict(np.array([[diff]]))[0]\n",
    "    else:\n",
    "        pred = 0.5\n",
    "    return pred\n",
    "\n",
    "# Create submission files for every combination.\n",
    "submission_folder = \"combined_submissions\"\n",
    "os.makedirs(submission_folder, exist_ok=True)\n",
    "combined_submission_files = []  # keep track of file names\n",
    "\n",
    "for m_name in men_best_models.keys():\n",
    "    for w_name in women_best_models.keys():\n",
    "        df_sub[\"Pred\"] = df_sub.apply(lambda row: predict_match(row, m_name, w_name), axis=1)\n",
    "        fname = os.path.join(submission_folder, f\"submission_{m_name}_{w_name}.csv\")\n",
    "        df_sub[[\"ID\", \"Pred\"]].to_csv(fname, index=False)\n",
    "        combined_submission_files.append(fname)\n",
    "        print(f\"Saved submission file: {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1204b-0146-4e67-bd05-66f150cbafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# For demonstration we simulate combined_preds_df.\n",
    "# In a real scenario, you would load all combined submission files (using pd.read_csv) \n",
    "# and align them by the \"ID\" order of the validation matches.\n",
    "# Here we assume that combined_preds_df has one column per combined submission.\n",
    "combined_preds_df = pd.DataFrame()\n",
    "for fname in combined_submission_files:\n",
    "    # Simulate predictions; in practice, load and merge on ID.\n",
    "    temp = pd.read_csv(fname)\n",
    "    col_name = os.path.basename(fname).replace(\"submission_\", \"\").replace(\".csv\", \"\")\n",
    "    combined_preds_df[col_name] = temp[\"Pred\"]\n",
    "\n",
    "# Define a function to compute ensemble RMSE and Brier score.\n",
    "def compute_ensemble_scores(pred_df, y_true, model_list, method=\"simple\", combined_rmse_dict=None):\n",
    "    if method == \"simple\":\n",
    "        ens_pred = pred_df[model_list].mean(axis=1)\n",
    "    elif method == \"weighted\":\n",
    "        # Use inverse RMSE weights (assume combined_rmse_dict contains each model's combined RMSE)\n",
    "        weights = np.array([1/combined_rmse_dict[m] for m in model_list])\n",
    "        weights = weights / np.sum(weights)\n",
    "        ens_pred = np.sum(pred_df[model_list].values * weights, axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"Method not supported.\")\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, ens_pred))\n",
    "    brier = np.mean((ens_pred - 1)**2)\n",
    "    return rmse, brier\n",
    "\n",
    "# For illustration, let’s create a dummy combined_rmse_dict by averaging the men's and women's RMSE.\n",
    "combined_rmse_dict = {}\n",
    "for m in men_results:\n",
    "    for w in women_results:\n",
    "        key = f\"{m}_{w}\"\n",
    "        combined_rmse_dict[key] = (men_results[m][\"rmse\"] + women_results[w][\"rmse\"]) / 2\n",
    "\n",
    "# Sort the combined models by RMSE.\n",
    "sorted_combined = sorted(combined_rmse_dict.items(), key=lambda x: x[1])\n",
    "sorted_combined_names = [x[0] for x in sorted_combined]\n",
    "\n",
    "ensemble_sizes = [2, 3, 5, 7, 9, 11]\n",
    "ensemble_results = []\n",
    "for size in ensemble_sizes:\n",
    "    models_used = sorted_combined_names[:size]\n",
    "    rmse_simple, brier_simple = compute_ensemble_scores(combined_preds_df, \n",
    "                                                        np.ones(len(combined_preds_df)), \n",
    "                                                        models_used, method=\"simple\")\n",
    "    rmse_weighted, brier_weighted = compute_ensemble_scores(combined_preds_df, \n",
    "                                                            np.ones(len(combined_preds_df)), \n",
    "                                                            models_used, method=\"weighted\",\n",
    "                                                            combined_rmse_dict=combined_rmse_dict)\n",
    "    ensemble_results.append({\n",
    "        \"Ensemble\": f\"top{size}\",\n",
    "        \"Method\": \"simple\",\n",
    "        \"RMSE\": rmse_simple,\n",
    "        \"Brier\": brier_simple\n",
    "    })\n",
    "    ensemble_results.append({\n",
    "        \"Ensemble\": f\"top{size}\",\n",
    "        \"Method\": \"weighted\",\n",
    "        \"RMSE\": rmse_weighted,\n",
    "        \"Brier\": brier_weighted\n",
    "    })\n",
    "\n",
    "ensemble_scores_df = pd.DataFrame(ensemble_results)\n",
    "print(\"Ensemble performance on combined predictions:\")\n",
    "print(ensemble_scores_df)\n",
    "\n",
    "# Plot ensemble RMSE vs ensemble size.\n",
    "plt.figure(figsize=(8,6))\n",
    "for method in [\"simple\", \"weighted\"]:\n",
    "    subset = ensemble_scores_df[ensemble_scores_df[\"Method\"]==method]\n",
    "    sizes = [int(x.replace(\"top\",\"\")) for x in subset[\"Ensemble\"]]\n",
    "    plt.plot(sizes, subset[\"RMSE\"], marker='o', label=method)\n",
    "plt.xlabel(\"Number of top combined models in ensemble\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Ensemble Performance on Combined Predictions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad73768c-177d-4665-b949-7353d4378019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Suppose we decide the best ensemble is the weighted ensemble with top3 models.\n",
    "best_ensemble = sorted_combined_names[:3]\n",
    "print(\"Best ensemble models (weighted, top3):\", best_ensemble)\n",
    "\n",
    "# Compute final ensemble prediction on the test set.\n",
    "# (In production, you would load your test set features, compute rating differences etc.)\n",
    "# For demonstration, we use our combined_preds_df (assuming it comes from the test set).\n",
    "final_weights = np.array([1/combined_rmse_dict[m] for m in best_ensemble])\n",
    "final_weights = final_weights / np.sum(final_weights)\n",
    "final_preds = np.sum(combined_preds_df[best_ensemble].values * final_weights, axis=1)\n",
    "\n",
    "# Create final submission DataFrame (assuming same order as df_sub_test)\n",
    "df_sub_test = pd.read_csv(os.path.join(data_folder, \"SampleSubmissionStage2.csv\"))\n",
    "df_sub_test[\"Pred\"] = final_preds\n",
    "submission_filename = \"final_submission_top3_weighted.csv\"\n",
    "df_sub_test[[\"ID\", \"Pred\"]].to_csv(submission_filename, index=False)\n",
    "print(f\"Final submission saved as {submission_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376f604d-dda6-4343-92cb-563048520eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e77e9d-d25d-4d39-b1a0-983955d8cdb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
