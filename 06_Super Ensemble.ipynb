{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b54dce3-836f-4059-a0a7-3a7fe192aec4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a85dc8b-9b9c-47fc-afed-33632ff3335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\", font_scale=1.1)\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fc914d8-bfb4-468d-b7db-f535d2c69612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. (Men’s and Women’s results, teams, and Massey ratings)\n"
     ]
    }
   ],
   "source": [
    "# Adjust these paths as needed\n",
    "data_folder = \"Input\"\n",
    "\n",
    "# Load men's regular season detailed results and teams\n",
    "m_reg = pd.read_csv(os.path.join(data_folder, \"MRegularSeasonDetailedResults.csv\"))\n",
    "MTeams = pd.read_csv(os.path.join(data_folder, \"MTeams.csv\"))\n",
    "\n",
    "# Load women's regular season detailed results and teams\n",
    "w_reg = pd.read_csv(os.path.join(data_folder, \"WRegularSeasonDetailedResults.csv\"))\n",
    "WTeams = pd.read_csv(os.path.join(data_folder, \"WTeams.csv\"))\n",
    "\n",
    "# Load Massey ratings – we assume it contains a column \"Team\" and \"MAS\"\n",
    "massey_ratings = pd.read_csv(\"Massey Ratings New.csv\")\n",
    "\n",
    "print(\"Data loaded. (Men’s and Women’s results, teams, and Massey ratings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "142d497e-af61-4f56-ae04-f0466239ae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New ratings computed for men and women (2019-2023).\n"
     ]
    }
   ],
   "source": [
    "def compute_team_new_rating(df, team_id, bonus=2):\n",
    "    \"\"\"\n",
    "    Computes the new rating for a team as the average adjusted margin.\n",
    "    Adjust margins using a bonus: if a team is home, add bonus; if away, subtract bonus.\n",
    "    \n",
    "    Parameters:\n",
    "      df (DataFrame): Game results (for seasons 2019-2023).\n",
    "      team_id (int): The team ID.\n",
    "      bonus (float): Points bonus for playing at home.\n",
    "      \n",
    "    Returns:\n",
    "      float: The new rating (average adjusted margin).\n",
    "    \"\"\"\n",
    "    margins = []\n",
    "    \n",
    "    # Games where team was the winner.\n",
    "    df_win = df[df[\"WTeamID\"] == team_id].copy()\n",
    "    df_win[\"adj_margin\"] = df_win[\"WScore\"] - df_win[\"LScore\"]\n",
    "    df_win.loc[df_win[\"WLoc\"] == \"H\", \"adj_margin\"] += bonus\n",
    "    df_win.loc[df_win[\"WLoc\"] == \"A\", \"adj_margin\"] -= bonus\n",
    "    margins.extend(df_win[\"adj_margin\"].tolist())\n",
    "    \n",
    "    # Games where team was the loser.\n",
    "    df_loss = df[df[\"LTeamID\"] == team_id].copy()\n",
    "    df_loss[\"adj_margin\"] = -(df_loss[\"WScore\"] - df_loss[\"LScore\"])\n",
    "    # For losing teams, if the winning team was home then the loser was away (further hurt),\n",
    "    # and vice versa.\n",
    "    df_loss.loc[df_loss[\"WLoc\"] == \"H\", \"adj_margin\"] -= bonus  \n",
    "    df_loss.loc[df_loss[\"WLoc\"] == \"A\", \"adj_margin\"] += bonus  \n",
    "    margins.extend(df_loss[\"adj_margin\"].tolist())\n",
    "    \n",
    "    if len(margins) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(margins)\n",
    "\n",
    "# Compute team IDs.\n",
    "team_ids_m = MTeams[\"TeamID\"].unique()\n",
    "team_ids_w = WTeams[\"TeamID\"].unique()\n",
    "\n",
    "# Filter results for seasons 2019-2023.\n",
    "m_reg_new = m_reg[(m_reg[\"Season\"] >= 2019) & (m_reg[\"Season\"] <= 2023)]\n",
    "w_reg_new = w_reg[(w_reg[\"Season\"] >= 2019) & (w_reg[\"Season\"] <= 2023)]\n",
    "\n",
    "# Compute new ratings.\n",
    "new_rating_men = {tid: compute_team_new_rating(m_reg_new, tid) for tid in team_ids_m}\n",
    "new_rating_women = {tid: compute_team_new_rating(w_reg_new, tid) for tid in team_ids_w}\n",
    "\n",
    "print(\"New ratings computed for men and women (2019-2023).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d010ccc8-9b29-424c-88de-c42c6f598d70",
   "metadata": {},
   "source": [
    "\n",
    "## Combine Ratings: Elo, New Rating, and Massey\n",
    "We assume that Elo ratings are provided (here dummy values of 1500 are used). We then map the Massey ratings (column \"MAS\")\n",
    "from the Massey ratings file to our teams and compute a weighted average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efe1af12-ccde-43fc-8ac5-034c42d5668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined ratings computed for men and women.\n"
     ]
    }
   ],
   "source": [
    "# For simplicity, suppose we already have dictionaries for Elo ratings:\n",
    "elo_m = {tid: 1500 for tid in team_ids_m}   # Replace with actual Elo ratings\n",
    "elo_w = {tid: 1500 for tid in team_ids_w}\n",
    "\n",
    "# Create a mapping from team name to MAS rating.\n",
    "massey_mapping = dict(zip(massey_ratings[\"Team\"], massey_ratings[\"MAS\"]))\n",
    "\n",
    "# Map MAS ratings onto the teams DataFrames.\n",
    "MTeams[\"CombinedMAS\"] = MTeams[\"TeamName\"].map(massey_mapping)\n",
    "WTeams[\"CombinedMAS\"] = WTeams[\"TeamName\"].map(massey_mapping)\n",
    "\n",
    "def compute_combined_rating(team_id, gender=\"M\", weight_elo=0.33, weight_new=0.33, weight_massey=0.34):\n",
    "    if gender == \"M\":\n",
    "        rating_elo = elo_m.get(team_id, 1500)\n",
    "        rating_new = new_rating_men.get(team_id, np.nan)\n",
    "        mas = MTeams.loc[MTeams[\"TeamID\"] == team_id, \"CombinedMAS\"]\n",
    "    else:\n",
    "        rating_elo = elo_w.get(team_id, 1500)\n",
    "        rating_new = new_rating_women.get(team_id, np.nan)\n",
    "        mas = WTeams.loc[WTeams[\"TeamID\"] == team_id, \"CombinedMAS\"]\n",
    "    rating_massey = mas.values[0] if not mas.empty else np.nan\n",
    "    ratings = np.array([rating_elo, rating_new, rating_massey], dtype=float)\n",
    "    weights = np.array([weight_elo, weight_new, weight_massey])\n",
    "    valid = ~np.isnan(ratings)\n",
    "    if valid.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.average(ratings[valid], weights=weights[valid])\n",
    "\n",
    "# Compute combined ratings.\n",
    "combined_rating_men = {tid: compute_combined_rating(tid, \"M\") for tid in team_ids_m}\n",
    "combined_rating_women = {tid: compute_combined_rating(tid, \"W\") for tid in team_ids_w}\n",
    "\n",
    "print(\"Combined ratings computed for men and women.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cd5ceef-d253-4dcc-afc5-1e1d4d048a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation data prepared for both men and women.\n"
     ]
    }
   ],
   "source": [
    "def prepare_model_data(df, combined_rating_dict, gender=\"M\"):\n",
    "    \"\"\"\n",
    "    Computes feature as the difference in combined ratings.\n",
    "    Target is the game margin (WScore - LScore).\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        if gender == \"M\":\n",
    "            r_w = combined_rating_men.get(row[\"WTeamID\"], np.nan)\n",
    "            r_l = combined_rating_men.get(row[\"LTeamID\"], np.nan)\n",
    "        else:\n",
    "            r_w = combined_rating_women.get(row[\"WTeamID\"], np.nan)\n",
    "            r_l = combined_rating_women.get(row[\"LTeamID\"], np.nan)\n",
    "        if np.isnan(r_w) or np.isnan(r_l):\n",
    "            continue\n",
    "        diff = r_w - r_l\n",
    "        X.append([diff])\n",
    "        y.append(row[\"WScore\"] - row[\"LScore\"])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Split data (use 2024 as validation)\n",
    "train_m = m_reg[m_reg[\"Season\"] < 2024]\n",
    "val_m   = m_reg[m_reg[\"Season\"] == 2024]\n",
    "train_w = w_reg[w_reg[\"Season\"] < 2024]\n",
    "val_w   = w_reg[w_reg[\"Season\"] == 2024]\n",
    "\n",
    "X_train_m, y_train_m = prepare_model_data(train_m, combined_rating_men, \"M\")\n",
    "X_val_m, y_val_m     = prepare_model_data(val_m, combined_rating_men, \"M\")\n",
    "X_train_w, y_train_w = prepare_model_data(train_w, combined_rating_women, \"W\")\n",
    "X_val_w, y_val_w     = prepare_model_data(val_w, combined_rating_women, \"W\")\n",
    "\n",
    "print(\"Training and validation data prepared for both men and women.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42ae62b5-73c3-4ce6-aeef-0183a2ac59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "def margin_to_probability(margin, scale=10.0):\n",
    "    \"\"\"\n",
    "    Convert a margin value into a win probability using a logistic transformation.\n",
    "    \n",
    "    \\[\n",
    "    p = \\frac{1}{1 + \\exp(-\\frac{\\text{margin}}{\\text{scale}})}\n",
    "    \\]\n",
    "    \n",
    "    Parameters:\n",
    "      margin (float or array-like): The predicted margin.\n",
    "      scale (float): Scaling factor to control steepness.\n",
    "      \n",
    "    Returns:\n",
    "      float or array-like: Win probability between 0 and 1.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1 + np.exp(-margin / scale))\n",
    "\n",
    "def tune_and_evaluate(model_obj, param_dist, X_train, y_train, X_val, y_val, scale=10.0):\n",
    "    \"\"\"\n",
    "    Tune the model using RandomizedSearchCV, then evaluate using RMSE on margin predictions and Brier score\n",
    "    on win probabilities.\n",
    "    \n",
    "    The ground truth win probability is computed as 1 if the true margin is > 0, else 0.\n",
    "    \"\"\"\n",
    "    rs = RandomizedSearchCV(model_obj, param_distributions=param_dist, \n",
    "                              n_iter=10, cv=3, scoring=\"neg_mean_squared_error\", \n",
    "                              random_state=42, error_score='raise', n_jobs=-1)\n",
    "    rs.fit(X_train, y_train)\n",
    "    best_model = rs.best_estimator_\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    \n",
    "    # Convert predictions and true margins to probabilities.\n",
    "    true_prob = (y_val > 0).astype(float)\n",
    "    pred_prob = margin_to_probability(y_pred, scale=scale)\n",
    "    brier = np.mean((pred_prob - true_prob)**2)\n",
    "    \n",
    "    return best_model, rmse, brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6931302-4998-4901-8294-767cef3cd55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, randint\n",
    "\n",
    "def build_dnn_model(layers=[Dense(32, activation='relu'), Dropout(0.2), Dense(1)]):\n",
    "    model = Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "models_grid = {\n",
    "    \"LinearRegression\": {\n",
    "        \"model\": LinearRegression(),\n",
    "        \"params\": {}\n",
    "    },\n",
    "    \"Ridge\": {\n",
    "        \"model\": Ridge(),\n",
    "        \"params\": {\"alpha\": uniform(0.1, 10)}\n",
    "    },\n",
    "    \"Lasso\": {\n",
    "        \"model\": Lasso(max_iter=5000),\n",
    "        \"params\": {\"alpha\": uniform(0.001, 1)}\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"model\": DecisionTreeRegressor(),\n",
    "        \"params\": {\"max_depth\": randint(1, 10), \"min_samples_split\": randint(2, 10)}\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        \"params\": {\"n_estimators\": randint(50, 200), \"max_depth\": randint(1, 10)}\n",
    "    },\n",
    "    \"ExtraTrees\": {\n",
    "        \"model\": ExtraTreesRegressor(random_state=42, n_jobs=-1),\n",
    "        \"params\": {\"n_estimators\": randint(50, 200), \"max_depth\": randint(1, 10)}\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"model\": GradientBoostingRegressor(random_state=42),\n",
    "        \"params\": {\"n_estimators\": randint(50, 200), \"learning_rate\": uniform(0.01, 0.3), \"max_depth\": randint(1, 10)}\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": xgb.XGBRegressor(tree_method=\"gpu_hist\", random_state=42, n_jobs=-1),\n",
    "        \"params\": {\"n_estimators\": randint(50, 200), \"learning_rate\": uniform(0.01, 0.3), \"max_depth\": randint(1, 10)}\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        \"model\": lgb.LGBMRegressor(device=\"gpu\", random_state=42),\n",
    "        \"params\": {\"n_estimators\": randint(50, 200), \"learning_rate\": uniform(0.01, 0.3), \"num_leaves\": randint(20, 50)}\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        \"model\": cb.CatBoostRegressor(task_type=\"GPU\", verbose=0, random_state=42),\n",
    "        \"params\": {\"iterations\": randint(50, 200), \"learning_rate\": uniform(0.01, 0.3), \"depth\": randint(3, 10)}\n",
    "    },\n",
    "    \"DNN\": {\n",
    "        \"model\": KerasRegressor(model=build_dnn_model, verbose=0),\n",
    "        \"params\": {\n",
    "            \"model__layers\": [[Dense(32, activation='relu'), Dropout(0.2), Dense(1)],\n",
    "                              [Dense(64, activation='relu'), Dropout(0.3), Dense(1)]],\n",
    "            \"batch_size\": randint(16, 64),\n",
    "            \"epochs\": randint(10, 50)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f623fdd-6d54-4ea9-8389-90cb4183d032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Men's models:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hi\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Men's LinearRegression: RMSE = 9.0281, Brier = 0.0540\n",
      "Men's Ridge: RMSE = 9.0281, Brier = 0.0540\n",
      "Men's Lasso: RMSE = 9.0297, Brier = 0.0540\n",
      "Men's DecisionTree: RMSE = 8.5177, Brier = 0.0562\n",
      "Men's RandomForest: RMSE = 8.5040, Brier = 0.0563\n",
      "Men's ExtraTrees: RMSE = 8.5953, Brier = 0.0556\n",
      "Men's GradientBoosting: RMSE = 8.5363, Brier = 0.0560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hi\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:05:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\Hi\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:05:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Men's XGBoost: RMSE = 8.5209, Brier = 0.0562\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 107634, number of used features: 1\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1 dense feature groups (0.41 MB) transferred to GPU in 0.000936 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 11.965123\n",
      "Men's LightGBM: RMSE = 8.5218, Brier = 0.0562\n"
     ]
    }
   ],
   "source": [
    "men_results = {}\n",
    "men_best_models = {}\n",
    "print(\"Tuning Men's models:\")\n",
    "for name, info in models_grid.items():\n",
    "    try:\n",
    "        best_model, rmse, brier = tune_and_evaluate(info[\"model\"], info[\"params\"], \n",
    "                                                    X_train_m, y_train_m, X_val_m, y_val_m)\n",
    "        men_results[name] = {\"rmse\": rmse, \"brier\": brier}\n",
    "        men_best_models[name] = best_model\n",
    "        print(f\"Men's {name}: RMSE = {rmse:.4f}, Brier = {brier:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Tuning men's {name} failed: {e}\")\n",
    "\n",
    "print(\"\\nTuning Women's models:\")\n",
    "women_results = {}\n",
    "women_best_models = {}\n",
    "for name, info in models_grid.items():\n",
    "    try:\n",
    "        best_model, rmse, brier = tune_and_evaluate(info[\"model\"], info[\"params\"], \n",
    "                                                    X_train_w, y_train_w, X_val_w, y_val_w)\n",
    "        women_results[name] = {\"rmse\": rmse, \"brier\": brier}\n",
    "        women_best_models[name] = best_model\n",
    "        print(f\"Women's {name}: RMSE = {rmse:.4f}, Brier = {brier:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Tuning women's {name} failed: {e}\")\n",
    "\n",
    "print(\"\\nMen's Model Results:\")\n",
    "print(men_results)\n",
    "print(\"\\nWomen's Model Results:\")\n",
    "print(women_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ee296-dca1-4ea2-a322-4b7d4326228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For men:\n",
    "val_preds_m = pd.DataFrame({\"ID\": np.arange(len(X_val_m))})\n",
    "for name, model in men_best_models.items():\n",
    "    val_preds_m[name] = model.predict(X_val_m)\n",
    "\n",
    "# For women:\n",
    "val_preds_w = pd.DataFrame({\"ID\": np.arange(len(X_val_w))})\n",
    "for name, model in women_best_models.items():\n",
    "    val_preds_w[name] = model.predict(X_val_w)\n",
    "\n",
    "print(\"Validation predictions generated for both men and women.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0456d-5d5f-4666-9751-d7c51764163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# For demonstration we simulate a small submission DataFrame.\n",
    "# In practice, load your actual submission file.\n",
    "df_sub = pd.DataFrame({\n",
    "    \"ID\": [\"2024_1101_1102\", \"2024_3101_3102\"],\n",
    "    \"Season\": [2024, 2024],\n",
    "    \"Team1\": [1101, 3101],\n",
    "    \"Team2\": [1102, 3102]\n",
    "})\n",
    "\n",
    "def predict_match(row, men_model_name, women_model_name):\n",
    "    s = row[\"Season\"]\n",
    "    t1 = row[\"Team1\"]\n",
    "    t2 = row[\"Team2\"]\n",
    "    # Use corresponding model predictions (here we use our tuned models on validation)\n",
    "    # In production, you would use the models to predict on your test features.\n",
    "    if t1 < 2000 and t2 < 2000:\n",
    "        # Use men’s prediction: we recompute feature diff using combined ratings.\n",
    "        diff = combined_rating_men.get(t1, 1500) - combined_rating_men.get(t2, 1500)\n",
    "        pred = men_best_models[men_model_name].predict(np.array([[diff]]))[0]\n",
    "    elif t1 >= 3000 and t2 >= 3000:\n",
    "        diff = combined_rating_women.get(t1, 1500) - combined_rating_women.get(t2, 1500)\n",
    "        pred = women_best_models[women_model_name].predict(np.array([[diff]]))[0]\n",
    "    else:\n",
    "        pred = 0.5\n",
    "    return pred\n",
    "\n",
    "# Create submission files for every combination.\n",
    "submission_folder = \"combined_submissions\"\n",
    "os.makedirs(submission_folder, exist_ok=True)\n",
    "combined_submission_files = []  # keep track of file names\n",
    "\n",
    "for m_name in men_best_models.keys():\n",
    "    for w_name in women_best_models.keys():\n",
    "        df_sub[\"Pred\"] = df_sub.apply(lambda row: predict_match(row, m_name, w_name), axis=1)\n",
    "        fname = os.path.join(submission_folder, f\"submission_{m_name}_{w_name}.csv\")\n",
    "        df_sub[[\"ID\", \"Pred\"]].to_csv(fname, index=False)\n",
    "        combined_submission_files.append(fname)\n",
    "        print(f\"Saved submission file: {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1204b-0146-4e67-bd05-66f150cbafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# For demonstration we simulate combined_preds_df.\n",
    "# In a real scenario, you would load all combined submission files (using pd.read_csv) \n",
    "# and align them by the \"ID\" order of the validation matches.\n",
    "# Here we assume that combined_preds_df has one column per combined submission.\n",
    "combined_preds_df = pd.DataFrame()\n",
    "for fname in combined_submission_files:\n",
    "    # Simulate predictions; in practice, load and merge on ID.\n",
    "    temp = pd.read_csv(fname)\n",
    "    col_name = os.path.basename(fname).replace(\"submission_\", \"\").replace(\".csv\", \"\")\n",
    "    combined_preds_df[col_name] = temp[\"Pred\"]\n",
    "\n",
    "# Define a function to compute ensemble RMSE and Brier score.\n",
    "def compute_ensemble_scores(pred_df, y_true, model_list, method=\"simple\", combined_rmse_dict=None):\n",
    "    if method == \"simple\":\n",
    "        ens_pred = pred_df[model_list].mean(axis=1)\n",
    "    elif method == \"weighted\":\n",
    "        # Use inverse RMSE weights (assume combined_rmse_dict contains each model's combined RMSE)\n",
    "        weights = np.array([1/combined_rmse_dict[m] for m in model_list])\n",
    "        weights = weights / np.sum(weights)\n",
    "        ens_pred = np.sum(pred_df[model_list].values * weights, axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"Method not supported.\")\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, ens_pred))\n",
    "    brier = np.mean((ens_pred - 1)**2)\n",
    "    return rmse, brier\n",
    "\n",
    "# For illustration, let’s create a dummy combined_rmse_dict by averaging the men's and women's RMSE.\n",
    "combined_rmse_dict = {}\n",
    "for m in men_results:\n",
    "    for w in women_results:\n",
    "        key = f\"{m}_{w}\"\n",
    "        combined_rmse_dict[key] = (men_results[m][\"rmse\"] + women_results[w][\"rmse\"]) / 2\n",
    "\n",
    "# Sort the combined models by RMSE.\n",
    "sorted_combined = sorted(combined_rmse_dict.items(), key=lambda x: x[1])\n",
    "sorted_combined_names = [x[0] for x in sorted_combined]\n",
    "\n",
    "ensemble_sizes = [2, 3, 5, 7, 9, 11]\n",
    "ensemble_results = []\n",
    "for size in ensemble_sizes:\n",
    "    models_used = sorted_combined_names[:size]\n",
    "    rmse_simple, brier_simple = compute_ensemble_scores(combined_preds_df, \n",
    "                                                        np.ones(len(combined_preds_df)), \n",
    "                                                        models_used, method=\"simple\")\n",
    "    rmse_weighted, brier_weighted = compute_ensemble_scores(combined_preds_df, \n",
    "                                                            np.ones(len(combined_preds_df)), \n",
    "                                                            models_used, method=\"weighted\",\n",
    "                                                            combined_rmse_dict=combined_rmse_dict)\n",
    "    ensemble_results.append({\n",
    "        \"Ensemble\": f\"top{size}\",\n",
    "        \"Method\": \"simple\",\n",
    "        \"RMSE\": rmse_simple,\n",
    "        \"Brier\": brier_simple\n",
    "    })\n",
    "    ensemble_results.append({\n",
    "        \"Ensemble\": f\"top{size}\",\n",
    "        \"Method\": \"weighted\",\n",
    "        \"RMSE\": rmse_weighted,\n",
    "        \"Brier\": brier_weighted\n",
    "    })\n",
    "\n",
    "ensemble_scores_df = pd.DataFrame(ensemble_results)\n",
    "print(\"Ensemble performance on combined predictions:\")\n",
    "print(ensemble_scores_df)\n",
    "\n",
    "# Plot ensemble RMSE vs ensemble size.\n",
    "plt.figure(figsize=(8,6))\n",
    "for method in [\"simple\", \"weighted\"]:\n",
    "    subset = ensemble_scores_df[ensemble_scores_df[\"Method\"]==method]\n",
    "    sizes = [int(x.replace(\"top\",\"\")) for x in subset[\"Ensemble\"]]\n",
    "    plt.plot(sizes, subset[\"RMSE\"], marker='o', label=method)\n",
    "plt.xlabel(\"Number of top combined models in ensemble\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Ensemble Performance on Combined Predictions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad73768c-177d-4665-b949-7353d4378019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Suppose we decide the best ensemble is the weighted ensemble with top3 models.\n",
    "best_ensemble = sorted_combined_names[:3]\n",
    "print(\"Best ensemble models (weighted, top3):\", best_ensemble)\n",
    "\n",
    "# Compute final ensemble prediction on the test set.\n",
    "# (In production, you would load your test set features, compute rating differences etc.)\n",
    "# For demonstration, we use our combined_preds_df (assuming it comes from the test set).\n",
    "final_weights = np.array([1/combined_rmse_dict[m] for m in best_ensemble])\n",
    "final_weights = final_weights / np.sum(final_weights)\n",
    "final_preds = np.sum(combined_preds_df[best_ensemble].values * final_weights, axis=1)\n",
    "\n",
    "# Create final submission DataFrame (assuming same order as df_sub_test)\n",
    "df_sub_test = pd.read_csv(os.path.join(data_folder, \"SampleSubmissionStage2.csv\"))\n",
    "df_sub_test[\"Pred\"] = final_preds\n",
    "submission_filename = \"final_submission_top3_weighted.csv\"\n",
    "df_sub_test[[\"ID\", \"Pred\"]].to_csv(submission_filename, index=False)\n",
    "print(f\"Final submission saved as {submission_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af7eb3-69c8-4581-8b8a-4dbbf3ed4752",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e77e9d-d25d-4d39-b1a0-983955d8cdb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
